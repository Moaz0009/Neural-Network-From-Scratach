{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7795633f-c166-4448-a174-9dbca170e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209d818d-c2fc-411f-bbba-e660e65ef67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=pd.DataFrame({'s' : [5],\n",
    "               't' :[7] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e39ddc-d28c-4806-bf84-2f4d66a0187b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array([5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b484f3da-aa89-4826-a895-84f286fef0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNeuralNetwork():\n",
    "    def __init__(self,layer_dims=(None),):\n",
    "        if not isinstance(layer_dims, tuple):\n",
    "            raise TypeError(\"Input 'layer_dims' must be of type tuple\")\n",
    "        self.layer_dims=layer_dims\n",
    "\n",
    "    def activation_functions (self,activations):\n",
    "        \"\"\"\n",
    "        activations : activation functions used for every layer; options include:\n",
    "            - linear\n",
    "            - relu\n",
    "            - sigmoid\n",
    "            - tanh\n",
    "            - softmax\n",
    "        \"\"\"\n",
    "        if not isinstance(activations, list):\n",
    "            raise TypeError(\"Input must be of type list\")\n",
    "        allowed_activ = set(['linear','relu', 'sigmoid', 'tanh', 'softmax'])\n",
    "        activ_set = set(activations)\n",
    "        diff = activ_set.difference(allowed_activ)\n",
    "        if(diff) :\n",
    "            raise InterruptedError(f'Invalid activation functions, allowed activations are {allowed_activ}')\n",
    "        L = len(self.layer_dims)\n",
    "        assert(len(activations)== L-1),\"Number of activations must equall number of layers\"\n",
    "        self.__activations = activations\n",
    "    def fit(self,X,Y,cost_function,print_cost=False,learning_rate=None,num_iterations=None,seed=None):\n",
    "        \"\"\"\n",
    "    Fits the model using the given data and parameters.\n",
    "    Parameters:\n",
    "    - X: Input features\n",
    "    - Y: Target values\n",
    "    - print_cost: Whether to print the cost during optimization\n",
    "    - learning_rate: The learning rate for optimization\n",
    "    - num_iterations: Number of iterations for optimization\n",
    "    - cost_function: The cost function to use; options include:\n",
    "        - \"MSE\" for Mean Squared Error (Regression)\n",
    "        - \"BinaryCrossEntropy\" for Binary Cross-Entropy (Binary Classification)\n",
    "        - \"CategoricalCrossEntropy\" for Categorical Cross-Entropy (Multi-Class Classification)\n",
    "        \"\"\"\n",
    "        X= np.array(X)\n",
    "        Y=np.array(Y)\n",
    "       # if (not isinstance(X, numpy.ndarray)) or (not isinstance(X, pandas.core.series.Series)) :\n",
    "         #   raise TypeError(\"Training data must be a pandas.series or numpy.ndarray\")\n",
    "       # if (not isinstance(Y, numpy.ndarray)) or (not isinstance(Y, pandas.core.series.Series)) :\n",
    "        #    raise TypeError(\"Training data must be a pandas.series or numpy.ndarray\")\n",
    "        if (X.shape[0] != self.layer_dims[0]):\n",
    "            raise IndentationError(f\"Expected an input of dimension ({self.layer_dims[0]},-), got ({X.shape[0]},-)\")\n",
    "        if (Y.shape[0] != self.layer_dims[-1]):\n",
    "            raise IndentationError(f\"Expected an input of dimension ({self.layer_dims[-1]},-), got ({Y.shape[0]},-)\")\n",
    "        self.X= X\n",
    "        self.Y= Y\n",
    "        self.__learning_rate = learning_rate\n",
    "        self.__num_iterations = num_iterations\n",
    "        self.__cost_function=cost_function\n",
    "        costs = []                         # keep track of cost\n",
    "        parameters  = intialise_parameters(layers_dims,seed)\n",
    "        print(parameters.keys())\n",
    "        for i in range(0, num_iterations):\n",
    "            AL,caches = forward_propagation(X, parameters,self.__activations)\n",
    "            cost = compute_cost(cost_function,AL,Y)\n",
    "            grads = back_propagation(AL, Y, caches,cost_function,self.__activations)\n",
    "            parameters = update_parameters(parameters,grads,learning_rate)\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost)      \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per hundreds)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        self.__parameters = parameters       \n",
    "    def predict(self,X, y):\n",
    "        m = X.shape[1]\n",
    "        n = len(self.__parameters) // 2 \n",
    "        p = np.zeros((1,m))\n",
    "        probas, caches = forward_propagation(X, self.__parameters,self.__activations)\n",
    "        for i in range(0, probas.shape[1]):\n",
    "            if probas[0,i] > 0.5:\n",
    "                p[0,i] = 1\n",
    "            else:\n",
    "                p[0,i] = 0\n",
    "        print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        self.predictions = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765b2a49-e9d0-4ba9-a6bf-ea4aea079c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward_(dA,cache):\n",
    "    Z = cache \n",
    "    dZ= dA * 1\n",
    "    return dZ\n",
    "def sigmoid_backward(dA,cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ\n",
    "def relu_backward(dA,cache):\n",
    "    Z=cache\n",
    "    dZ = dA\n",
    "    print('relu back :','Z' , Z.shape ,'dZ' ,  dZ.shape)\n",
    "    dZ[dA<=0]=0\n",
    "    return dZ\n",
    "def tanh_backward(dA,cache):\n",
    "    Z=cache\n",
    "    tanh_Z= (np.exp(Z) - np.exp(-1*Z))/(np.exp(Z) + np.exp(-1*Z))\n",
    "    dZ = dA * 1-tanh_Z**2\n",
    "    return dZ\n",
    "def softmax_backward(dA,cache,Y):\n",
    "    Z=cache\n",
    "    e_x = np.exp(Z- np.max(Z))  # Subtract max(Z) for numerical stability\n",
    "    props= e_x / e_x.sum(axis=0) \n",
    "    dZ= ( props - Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a0444ff-c75e-454a-9038-09203759db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialise_parameters(layers_dim,seed):\n",
    "    if(seed):\n",
    "        np.random.seed(seed)\n",
    "    L=len(layers_dim)\n",
    "    parameters={}\n",
    "    for l in range(1,L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dim[l],layers_dim[l-1]) * np.sqrt(1/layers_dim[l-1])\n",
    "        parameters['b'+ str(l)] = np.zeros((layers_dim[l],1))\n",
    "        print('W' + str(l) , f'  {(layers_dim[l],layers_dim[l-1])}')\n",
    "        # Check the dimensions\n",
    "        assert(parameters['W' + str(l)].shape == (layers_dim[l], layers_dim[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layers_dim[l], 1))        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d58c13ff-de96-4932-b9b3-c7d7a5c8cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(Z):\n",
    "    return Z\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-1*Z))\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "def tanh(Z):\n",
    "    return (np.exp(Z) - np.exp(-1*Z))/(np.exp(Z) + np.exp(-1*Z))\n",
    "def softmax(Z):\n",
    "    e_x = np.exp(Z- np.max(Z))  # Subtract max(Z) for numerical stability\n",
    "    return e_x / e_x.sum(axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06dfe46-b7ab-4e34-bb81-f59d81141475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_Backward(A,Y):\n",
    "    return 2*(A-Y)\n",
    "def BCE_Backward(A,Y):\n",
    "    epsilon = 1e-10  # small value to avoid division by zero\n",
    "    A = np.clip(A, epsilon, 1 - epsilon)\n",
    "    return  - (np.divide(Y, A) - np.divide(1 - Y, 1 - A)) \n",
    "def CCE_Backward(A,Y):\n",
    "    epsilon = 1e-10  # small value to avoid division by zero\n",
    "    A = np.clip(A, epsilon, 1 - epsilon)\n",
    "    return  - np.divide(Y, A) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cbc30e5-0dad-4ffc-a731-344ca903a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_activation(A_prev,W,b,activation):\n",
    "    Z=np.dot(W,A_prev) + b\n",
    "    if(activation == 'relu'):\n",
    "        A=relu(Z)\n",
    "    elif(activation=='sigmoid'):\n",
    "        A= sigmoid(Z)\n",
    "    elif(activation=='tanh'):\n",
    "        A=tanh(Z)\n",
    "    elif(activation=='softmax'):\n",
    "        A=softmax(Z)\n",
    "    else:\n",
    "        A=lineare(Z)\n",
    "    linear_cache=(A,W,b)\n",
    "    activation_cache=Z\n",
    "    cache=(linear_cache,activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6f4124a-f1a6-4457-a935-5ddae0997980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters,activations):\n",
    "    L=len(parameters) // 2 #number of layers\n",
    "    A=X\n",
    "    caches=[]\n",
    "    for l in range(1,L):\n",
    "        A_prev=A\n",
    "        print('forward : ','A_prev ' ,A_prev.shape)\n",
    "        A,cache = linear_forward_activation(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],activations[l-1])\n",
    "        caches.append(cache)\n",
    "    A , cache = linear_forward_activation(A,parameters['W'+str(L)], parameters['b'+str(L)], activations[L-1])\n",
    "    caches.append(cache)\n",
    "    return A,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49410d95-0715-4e55-8091-43c89966ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(A,Y):\n",
    "    return  np.sum((A-Y)**2)\n",
    "def BCE (A,Y):\n",
    "    epsilon=1e-15\n",
    "    A = np.clip(A, epsilon, 1 - epsilon)  # Clip values to avoid log(0)\n",
    "    return -1*(Y * np.log(A) + (1-Y) * np.log(1-A))\n",
    "def CCE (A,Y):\n",
    "    epsilon=1e-15\n",
    "    A = np.clip(A, epsilon, 1 - epsilon)  # Clip values to avoid log(0)\n",
    "    return -1 * np.sum(Y *log(A) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be72167-5de8-487d-8151-4e95247a5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(function,A,Y):\n",
    "    m=Y.shape[1]\n",
    "    if(function.lower() =='mse'):\n",
    "       return (1/(2*m)) * MSE(A,Y)\n",
    "    elif(function.lower() == 'binarycrossentropy'):\n",
    "       return (1/(2*m)) * BCE(A,Y)   \n",
    "    elif(function.lower() == 'categoricalcrossentropy'):\n",
    "       return (1/(2*m)) * CCE(A,Y)\n",
    "    else:\n",
    "        raise ImportError(\"Invalid Cost function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08347ecf-b1c2-42d2-88c2-af91320e11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache): \n",
    "    A_prev, W, b = cache\n",
    "  #  A_prev (1, 209) W (1, 5) B (1, 1)\n",
    "    print('A_prev' , A_prev.shape,'W', W.shape,'B', b.shape,'dZ',dZ.shape)\n",
    "    m = A_prev.shape[1]\n",
    "    dA_prev = (W.T @ dZ)\n",
    "    dW = ( dZ @ A_prev.T) /m\n",
    "    db = np.sum(dZ , axis=1 ,keepdims=True)/m\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f474e394-5ae7-4e1d-b23f-7a01a4152027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if(activation == 'relu'):\n",
    "        dZ=relu_backward(dA,activation_cache)\n",
    "    elif(activation=='sigmoid'):\n",
    "        dZ= sigmoid_backward(dA,activation_cache)\n",
    "    elif(activation=='tanh'):\n",
    "        dZ=tanh_backward(dA,activation_cache)\n",
    "    elif(activation=='softmax'):\n",
    "        dZ=softmax_backward(dA,activation_cache)\n",
    "    else:\n",
    "        dZ=linear_backward_(dA)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    print('dW' , dW.shape,'dA_prev' , dA_prev.shape)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35667bc4-3855-4563-a100-152bd8dbe212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(A,Y,caches,cost_func,activiations):\n",
    "    Y = Y.reshape(A.shape) # after this line, Y is the same shape as A\n",
    "    if(cost_func.lower() =='mse'):\n",
    "       dA = MSE_Backward(A,Y)\n",
    "    elif(cost_func.lower() == 'binarycrossentropy'):\n",
    "       dA = BCE_Backward(A,Y)\n",
    "    elif(cost_func.lower() == 'categoricalcrossentropy'):\n",
    "       dA = CCE_Backward(A,Y)\n",
    "    grads={}\n",
    "    L=len(caches)\n",
    "    m = A.shape[1]\n",
    "    print('Layers ' , L)\n",
    "    grads[\"dA\"+str(L-1)], grads[\"dW\" + str(L)], grads[\"db\"+str(L)] = linear_activation_backward(dA , caches[L-1] , activiations[-1])\n",
    "    for l in reversed(range(1,L-1)):\n",
    "        grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] = linear_activation_backward(grads[\"dA\" +str(l+1)] , caches[l+1] , activiations[l])\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48045ce9-1169-4b2d-b70e-71fb125996e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(1,L+1):\n",
    "        parameters['W' + str(l)].T = parameters['W' + str(l)].T - learning_rate * grads['dW' + str(l) ]\n",
    "        parameters['b' + str(l)] = parameters['b' + str(l)] - learning_rate * grads['db' + str(l) ]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e279b2f1-6a2e-4b91-854d-f390fa8d3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec90da81-962d-4042-8839-7fd9d9a2e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08ed7411-5d18-4ef1-ae9e-14012e6e2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = (12288, 20, 7, 5, 1) #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "963404d6-06d1-4ba2-b5da-a5a1360c64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DenseNeuralNetwork(layer_dims=layers_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "061a3b8f-fe88-4956-9143-259e8466b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn.activation_functions(['relu','relu','relu','sigmoid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84350852-4209-488a-82a7-957a4a00e02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1   (20, 12288)\n",
      "W2   (7, 20)\n",
      "W3   (5, 7)\n",
      "W4   (1, 5)\n",
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4'])\n",
      "forward :  A_prev  (12288, 209)\n",
      "forward :  A_prev  (20, 209)\n",
      "forward :  A_prev  (7, 209)\n",
      "Layers  4\n",
      "A_prev (1, 209) W (1, 5) B (1, 1) dZ (1, 209)\n",
      "dW (1, 1) dA_prev (5, 209)\n",
      "relu back : Z (1, 209) dZ (5, 209)\n",
      "A_prev (1, 209) W (1, 5) B (1, 1) dZ (5, 209)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dnn\u001b[38;5;241m.\u001b[39mfit(train_x,train_y,cost_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBinaryCrossEntropy\u001b[39m\u001b[38;5;124m'\u001b[39m,print_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,learning_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0075\u001b[39m,num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3000\u001b[39m,seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 61\u001b[0m, in \u001b[0;36mDenseNeuralNetwork.fit\u001b[1;34m(self, X, Y, cost_function, print_cost, learning_rate, num_iterations, seed)\u001b[0m\n\u001b[0;32m     59\u001b[0m AL,caches \u001b[38;5;241m=\u001b[39m forward_propagation(X, parameters,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__activations)\n\u001b[0;32m     60\u001b[0m cost \u001b[38;5;241m=\u001b[39m compute_cost(cost_function,AL,Y)\n\u001b[1;32m---> 61\u001b[0m grads \u001b[38;5;241m=\u001b[39m back_propagation(AL, Y, caches,cost_function,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__activations)\n\u001b[0;32m     62\u001b[0m parameters \u001b[38;5;241m=\u001b[39m update_parameters(parameters,grads,learning_rate)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_cost \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m, in \u001b[0;36mback_propagation\u001b[1;34m(A, Y, caches, cost_func, activiations)\u001b[0m\n\u001b[0;32m     13\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L)], grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(L)] \u001b[38;5;241m=\u001b[39m linear_activation_backward(dA , caches[L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] , activiations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 15\u001b[0m     grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l)] , grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)] , grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m=\u001b[39m linear_activation_backward(grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)] , caches[l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] , activiations[l])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m, in \u001b[0;36mlinear_activation_backward\u001b[1;34m(dA, cache, activation)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     dZ\u001b[38;5;241m=\u001b[39mlinear_backward_(dA)\n\u001b[1;32m---> 13\u001b[0m dA_prev, dW, db \u001b[38;5;241m=\u001b[39m linear_backward(dZ, linear_cache)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m'\u001b[39m , dW\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdA_prev\u001b[39m\u001b[38;5;124m'\u001b[39m , dA_prev\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dA_prev, dW, db\n",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m, in \u001b[0;36mlinear_backward\u001b[1;34m(dZ, cache)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA_prev\u001b[39m\u001b[38;5;124m'\u001b[39m , A_prev\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m, W\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, b\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdZ\u001b[39m\u001b[38;5;124m'\u001b[39m,dZ\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m m \u001b[38;5;241m=\u001b[39m A_prev\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m dA_prev \u001b[38;5;241m=\u001b[39m (W\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m dZ)\n\u001b[0;32m      7\u001b[0m dW \u001b[38;5;241m=\u001b[39m ( dZ \u001b[38;5;241m@\u001b[39m A_prev\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m/\u001b[39mm\n\u001b[0;32m      8\u001b[0m db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ , axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m ,keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m/\u001b[39mm\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 1)"
     ]
    }
   ],
   "source": [
    "dnn.fit(train_x,train_y,cost_function='BinaryCrossEntropy',print_cost=True,learning_rate= 0.0075,num_iterations = 3000,seed=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
